{"data":{"markdownRemark":{"html":"<p>블로그에서는 태그를 없애야 함.</p>\n<h2 id=\"tags\"><a href=\"#tags\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tags:</h2>\n<h1 id=\"meta\"><a href=\"#meta\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>meta</h1>","fields":{"slug":"processor-수정방안"},"frontmatter":{"title":"processor 수정방안","date":"06.06.2019","category":"meta","tags":["meta"],"banner":"/assets/bg/2.jpg"},"timeToRead":1}},"pageContext":{"slug":"processor-수정방안","prev":{"excerpt":"edwith강의 시청\nhttps://www.edwith.org/machinelearning1_17/lecture/10575/MLE: Maximum Likelihood Estimation최적의 가능성을 가지는 를 추론하는것..?단조 함수라서, 로그를 취해서 계산을 편하게 함.이 값을 최대화해야하므로, 미분이 0이 되는 위치를 찾는다.가 일 때 MLE 관점에서 최적.Simple Error Bound…","html":"<p>edwith강의 시청\n<a href=\"https://www.edwith.org/machinelearning1_17/lecture/10575/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://www.edwith.org/machinelearning1_17/lecture/10575/</a></p>\n<h3 id=\"mle-maximum-likelihood-estimation\"><a href=\"#mle-maximum-likelihood-estimation\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>MLE: Maximum Likelihood Estimation</h3>\n<p>최적의 가능성을 가지는 $\\theta$를 추론하는것..?</p>\n<p>$\\hat\\theta=argmax_\\theta P(D|\\theta)$</p>\n<p>$\\hat\\theta=argmax_\\theta P(D|\\theta)=argmax_\\theta \\theta^{a_H}(1-\\theta)^{a_T}$</p>\n<p>단조 함수라서, 로그를 취해서 계산을 편하게 함.</p>\n<p>$\\hat\\theta=argmax_\\theta lnP(D|\\theta)=argmax_\\theta ln{\\theta^{a_H}(1-\\theta)^{a_T}}=argmax_\\theta \\{a_Hln\\theta+a_Tln(1-\\theta)\\}$</p>\n<p>이 값을 최대화해야하므로, 미분이 0이 되는 위치를 찾는다.</p>\n<p>$\\frac{d}{d\\theta}(a_Hln\\theta+a_Tln(1-\\theta))=0\n\\newline\n\\frac{a_H}{\\theta}-\\frac{a_T}{1-\\theta}=0\n\\newline\n\\theta=\\frac{a_H}{a_T+a_H}$</p>\n<p>$\\theta$가 $\\frac{a_H}{a_T+a_H}$일 때 MLE 관점에서 최적.</p>\n<h3 id=\"simple-error-bound\"><a href=\"#simple-error-bound\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Simple Error Bound</h3>\n<p>실제 던졌을 때의 $\\theta$에 대한 최적 $\\hat\\theta$와의 에러 $\\epsilon$?</p>\n<p>Hoeffding's inequality</p>\n<p>$P(|\\hat\\theta-\\theta^*|\\ge\\epsilon)\\le2e^{-2N\\epsilon^2}$</p>\n<p>이 식으로, 에러 0.1 이하의 경우가 0.01%가 되도록 하기 위한 N을 구한다던가 하는것이 가능.</p>\n<p>Probably Approximate Correct learning (PAC)\n(case, error)</p>\n<h2 id=\"tags\"><a href=\"#tags\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tags:</h2>\n<h1 id=\"study-machine-learning\"><a href=\"#study-machine-learning\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>study, #machine-learning</h1>","id":"dc6f904e-4d5b-5c04-8788-3be5b446837d","fields":{"slug":"딥러닝-공부"},"frontmatter":{"date":"2019-06-06","title":"딥러닝 공부.","category":"blog","tags":["blog"],"banner":"/assets/bg/3.jpg"},"timeToRead":1},"next":{"excerpt":"","html":"","id":"a064fcbf-1be0-5a11-b7ea-887a21f5ffa1","fields":{"slug":""},"frontmatter":{"date":null,"title":"","category":null,"tags":null,"banner":null},"timeToRead":1}}}