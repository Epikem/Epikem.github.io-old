{"data":{"markdownRemark":{"html":"<p>skaffold 예제의 microservices example 스키마를 고쳐서 사용함.\n<code class=\"language-text\">skaffold dev</code>로 올리고 나서 gcp 포트 포워딩 후 웹 미리보기로 봤는데, 클라이언트가 통신 시도는 하는거 같은데 http https 섞여서 막힘. (클라이언트는 <a href=\"https://8080-..\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">https://8080-..</a>. 이런 주소였고, 서버는 http). 그래서 인증서 설치하고 express에 https 적용하려고 openssl로 인증서를 만드려 했는데 뭔가 config file이 없다는 내용의 에러가 났다. <code class=\"language-text\">scoop which openssl</code>로 확인해 보니 <code class=\"language-text\">miniconda</code>의 openssl이었다. 다시 <code class=\"language-text\">scoop install openssl</code>로 설치한 후, openssl로 cert 만들어서 루트 폴더에 <code class=\"language-text\">cert</code>폴더를 만들고, 키와 cert파일을 넣고 도커파일에서 <code class=\"language-text\">COPY ../cert ./cert</code>이렇게 카피하려고 했더니 빌드 컨텍스트 밖이라며 에러가 났다. 그래서 좀 고민하다가, 알고보니 그냥 로드밸런서로 외부 노출해서 접속하면 클라이언트 주소가 https가 아니라 http여서 통신이 되었다. 그리고 제대로 검색까지 되었다. 드디어 클러스터 전체를 올린 것이다.</p>\n<p>이제 로드 밸런서로 노출된 클라이언트 주소를 aws 도메인이랑 연결하려고 했는데, 또 invalid host header가 뜨면서 실패했다. </p>\n<p><code class=\"language-text\">.env</code>파일에 HOST 환경 변수를 넣어서 <code class=\"language-text\">vocabo.epikem.com</code>로도 해보고, 로드 밸런서 주소로도 해보고, 포트 쌍도 <code class=\"language-text\">80</code>과 <code class=\"language-text\">3000</code> 두 가지로 네 조합을 다 돌려봤으나 전부 실패했다. open port를 찾을 수 없다거나,EADDRNOTAVAIL 이런 에러가 난다. <code class=\"language-text\">disableHostCheck</code> 아니면 직접 빌드하는것 밖에는 답이 없는걸까? 클라이언트를 빌드하고 <code class=\"language-text\">serve</code>로 웹 서비스를 제공하는 절차를 도커화 해야하려나? 그렇게 하면 연결이 되긴 할까? ~.~ 그리고 이렇게 하면 매 시도마다 빌드를 해야 해서 또 시간 깨나 먹을거 같은데.. 아니면 빌드해서 s3에 그냥 정적 페이지로 올려도 아마 될거같은데.</p>\n<p>어쩌면 <code class=\"language-text\">package.json</code>파일의 <code class=\"language-text\">homepage</code> 필드로 할 수 있는 부분이었을지도.</p>\n<h2 id=\"tags\"><a href=\"#tags\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tags</h2>\n<p>  #vocabo, #cloud</p>","fields":{"slug":"vocabo-cluster-동작-성공"},"frontmatter":{"title":"vocabo-cluster 동작 성공","date":"27.05.2019","category":"vocabo","tags":["project/vocabo"],"banner":"/assets/bg/2.jpg"},"timeToRead":1}},"pageContext":{"slug":"vocabo-cluster-동작-성공","prev":{"excerpt":"에러가 나는게,  여기  설명에 나온대로 elasticsearch log를 확인해 보니, jaso analyzer 관련 클래스인  에서  에러가 나고 있었다. 들어가 코드를 확인해 보니, vscode에서  콜에서 매개변수가 안맞다며 빨간 줄이 쳐지고 quick fix를 하니 부모의 시그니쳐에 맞게 한 매개변수가 삭제되었다. 삭제하고 빌드해서 s3에 올리고 원래 설치된것 삭제하고 url…","html":"<p>에러가 나는게, <a href=\"https://docs.bitnami.com/google/apps/elasticsearch/get-started/understand-default-config/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">여기</a> 설명에 나온대로 elasticsearch log를 확인해 보니, jaso analyzer 관련 클래스인 <code class=\"language-text\">JasoTokenizerFactory</code>에서 <code class=\"language-text\">NoSuchMethod</code>에러가 나고 있었다. 들어가 코드를 확인해 보니, vscode에서 <code class=\"language-text\">super(...)</code>콜에서 매개변수가 안맞다며 빨간 줄이 쳐지고 quick fix를 하니 부모의 시그니쳐에 맞게 한 매개변수가 삭제되었다. 삭제하고 빌드해서 s3에 올리고 원래 설치된것 삭제하고 url로 다시 설치했는데 또 실패하길래 왜지 하다가 생각해보니 또 elasticsearch 서비스 재시작이 필요한 것이었다. 서비스 재시작을 하니 드디어 성공적으로 kengdic 인덱스를 만들었다. 아 그리고 매핑 타입이 없어졌다는걸 봐서 일단 'logs' 타입을 빼고 PUT 요청을 넣었다. 전에 쓰던 인덱스 세팅이 두 가지 있는데 뭐였는지 까먹었다. 아마도 아래꺼일듯.</p>\n<p>최종 요청 (아래꺼 씀.)</p>\n<div class=\"gatsby-highlight\" data-language=\"json\"><pre class=\"language-json\"><code class=\"language-json\">PUT kengdic\n<span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"settings\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"analysis\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"nori-user-dict\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"nori_tokenizer\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"decompound_mode\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"mixed\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"user_dictionary\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"userdict_ko.txt\"</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"analyzer\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"analyzer-kor\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"custom\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"nori-user-dict\"</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"analyzer-eng\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"custom\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"standard\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"filter\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">[</span>\n            <span class=\"token string\">\"lowercase\"</span>\n          <span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"mappings\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n    // <span class=\"token property\">\"logs\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"properties\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"word\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          // <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"text\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"completion\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"analyzer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"analyzer-kor\"</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"def\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          // <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"text\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"completion\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"analyzer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"analyzer-eng\"</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span>\n    // <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span>\n\nPUT kengdic\n<span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"settings\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"analysis\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"filter\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"suggest_filter\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"edge_ngram\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"min_gram\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"max_gram\"</span><span class=\"token operator\">:</span> <span class=\"token number\">50</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"nori-user-dict\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"nori_tokenizer\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"decompound_mode\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"mixed\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"user_dictionary\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"userdict_ko.txt\"</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"jaso_search_tokenizer\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"jaso_tokenizer\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"mistype\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"chosung\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">false</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"jaso_index_tokenizer\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"jaso_tokenizer\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"mistype\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"chosung\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span>\n        <span class=\"token punctuation\">}</span>\n        // <span class=\"token property\">\"jaso_search_tokenizer\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        //   <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"nori_tokenizer\"</span><span class=\"token punctuation\">,</span>\n        //   <span class=\"token property\">\"mistype\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n        //   <span class=\"token property\">\"chosung\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">false</span>\n        // <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        // <span class=\"token property\">\"jaso_index_tokenizer\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        //   <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"nori_tokenizer\"</span><span class=\"token punctuation\">,</span>\n        //   <span class=\"token property\">\"mistype\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n        //   <span class=\"token property\">\"chosung\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span>\n        // <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"analyzer\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"analyzer-kor\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"custom\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"nori-user-dict\"</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"analyzer-eng\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"custom\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"standard\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"filter\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">[</span>\n            <span class=\"token string\">\"lowercase\"</span>\n          <span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"kor-suggest_search_analyzer\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"custom\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"jaso_search_tokenizer\"</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"kor-suggest_index_analyzer\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"custom\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"tokenizer\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"jaso_index_tokenizer\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"filter\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n            <span class=\"token string\">\"suggest_filter\"</span>\n          <span class=\"token punctuation\">]</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"mappings\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"logs\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"properties\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"word\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"text\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"store\"</span><span class=\"token operator\">:</span><span class=\"token boolean\">true</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"analyzer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"kor-suggest_index_analyzer\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"search_analyzer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"kor-suggest_search_analyzer\"</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"def\"</span><span class=\"token operator\">:</span><span class=\"token punctuation\">{</span>\n          // <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"text\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"completion\"</span><span class=\"token punctuation\">,</span>\n          <span class=\"token property\">\"analyzer\"</span><span class=\"token operator\">:</span><span class=\"token string\">\"analyzer-eng\"</span>\n        <span class=\"token punctuation\">}</span>\n      <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre></div>\n<p>그런데 데이터를 다 넣어도 <code class=\"language-text\">GET _stats</code>의 결과에서 kengdic의 document count가 0이길래 왜인가 싶어 로그를 확인했더니, <code class=\"language-text\">rejecting mapping update to as the final mapping would have more than 1 type [_doc, logs]</code> 이런 에러가 떴다. 생각해보니, elasticsearch 버전 7쯤에서 매핑 타입이 없어졌다지만, 나는 scoop으로 설치한 구버전 logstash를 그대로 쓰고있어서 type을 지정 안 하면 전처럼 <code class=\"language-text\">logs</code> 타입으로 기본으로 넣으려 한 것이다. 그래서 logstash 문서에서 찾아보니 elasticsearch output 플러그인에서는 <code class=\"language-text\">document_type</code>으로 지정하면 된다고 해서 <code class=\"language-text\">document_type=&gt;&quot;_doc&quot;</code>이렇게 하니 성공적으로 데이터가 들어가고 검색도 성공했다. 드디어!</p>\n<p>드디어 클러스터 전체를 테스트할 차례가 왔다. 문제는 어디 가거나 잘 때마다 이 전체 설정을 반복해야하는데.. bash 스크립트로 짜야 하려나?</p>\n<p>elasticsearch 서비스 생성 과정:</p>\n<ol>\n<li>elasticsearch cluster certified by bitnami를 배포.</li>\n<li>배포 완료되면 elasticsearch-vm에 접속</li>\n<li>vm에서 <code class=\"language-text\">jaso-analyzer</code>, <code class=\"language-text\">nori-analysis</code> 플러그인 설치. (<code class=\"language-text\">jaso-analyzer</code>는 aws s3에 올려놓고 퍼블릭으로 설정하면 주소로 바로 설치 가능.)</li>\n<li>vm에서 <code class=\"language-text\">sudo touch /opt/bitnami/elasticsearch/config/userdict_ko.txt</code> 로 사용자 사전 파일 생성</li>\n<li>vm에서 <code class=\"language-text\">sudo /opt/bitnami/ctlscript.sh restart elasticsearch</code>로 es 서비스 재시작(플러그인 로드)</li>\n<li>es 서비스가 재시작되었으면 <code class=\"language-text\">kengdic</code> 인덱스 생성하기.</li>\n<li>로컬에서 logstash의 elasticsearch output 필터의 <code class=\"language-text\">user</code>, <code class=\"language-text\">password</code> 항목을 위에서 생성한 클러스터의 정보를 참고하여 업데이트</li>\n<li><code class=\"language-text\">importDatas.ps1</code> 실행</li>\n</ol>\n<p>로컬과 구글 콘솔 gui와 원격 vm 환경이 조합되어있어 자동화가 어려워보인다.. gcp에서 현재 상태를 다시 docker image처럼 템플릿화해서 저장해둘 수 있으면 정말 좋겠다. </p>\n<h2 id=\"tags\"><a href=\"#tags\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tags</h2>\n<p>  #vocabo, #elasticsearch</p>","id":"c3dc6808-2c5f-5135-b897-607aa7601ab0","fields":{"slug":"드디어-kengdic-index-생성-완료"},"frontmatter":{"date":"2019-05-26","title":"드디어!! kengdic index 생성 완료","category":"vocabo","tags":["project/vocabo"],"banner":"/assets/bg/3.jpg"},"timeToRead":2},"next":{"excerpt":"aws Lambda로 전환 완료, https 적용 및 react devtools버그 해결(크롬 렌더링 문제). aws Lambda로  에 서버를 올렸는데 에러가 난 이유를 못찾아서 로그를 어디서 찾나 했는데 aws Lambda 다이어그램에서 확인해보니 aws CloudWatch에 기록되고 있었다. 로그를 확인해 보니 app.js 파일 1…","html":"<p>aws Lambda로 전환 완료, https 적용 및 react devtools버그 해결(크롬 렌더링 문제).</p>\n<p>aws Lambda로 <code class=\"language-text\">vocabo-api.epikem.com</code>에 서버를 올렸는데 에러가 난 이유를 못찾아서 로그를 어디서 찾나 했는데 aws Lambda 다이어그램에서 확인해보니 aws CloudWatch에 기록되고 있었다. 로그를 확인해 보니 app.js 파일 15번째 줄에서 컴파일 에러가 났는데, <code class=\"language-text\">new URL(&#39;...&#39;)</code> 부분이었다. 노드에서는 직접 <code class=\"language-text\">url</code> 모듈을 로드하지 않아도 코드가 돌아갔었는데, 람다에서는 로드를 해야하는 것이었다. 다음과 같이 <code class=\"language-text\">url</code>모듈의 <code class=\"language-text\">URL</code>을 로드하여 사용하니 이 부분은 해결되었다.</p>\n<div class=\"gatsby-highlight\" data-language=\"js\"><pre class=\"language-js\"><code class=\"language-js\"><span class=\"token keyword\">const</span> <span class=\"token punctuation\">{</span> <span class=\"token constant\">URL</span> <span class=\"token punctuation\">}</span> <span class=\"token operator\">=</span> <span class=\"token function\">require</span><span class=\"token punctuation\">(</span><span class=\"token string\">'url'</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">;</span></code></pre></div>\n<p>다시 패키지 빌드 및 배포를 시도하니 이번엔 url이 잘못됬다는 에러가 떴는데, 예상한 대로 환경변수 문제였다. <code class=\"language-text\">.env</code>파일 추가로는 람다 패키지 추가가 되지 않았고, 다행히 AWS CloudFormation 공식문서에서 Lambda에 환경변수를 설정하는 방법을 찾아 다음과 같이 채운 다음 다시 배포하니 성공적으로 api 서버가 등록되었다. 드디어 https 및 커스텀 도메인이 적용된 api 서버를 만들었다. 게다가 호출한 만큼만 사용되므로 절약까지 된다. 다만 클라우드 저장소는 결국 샘플에서 수정만 한 것인데, 직접 이런 템플릿을 짜라 하면 도저히 못 할거 같은 느낌이 든다.. (하지만 이건 aws&#x26;cloudformation 쪽이니 모르는게 당연한 것이다)</p>\n<p>file: cloudformation.yaml</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">vocaboLambda</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">...</span>\n  <span class=\"token key atrule\">Properties</span><span class=\"token punctuation\">:</span>\n    <span class=\"token punctuation\">...</span>\n    <span class=\"token key atrule\">Environment</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">Variables</span><span class=\"token punctuation\">:</span>\n        <span class=\"token key atrule\">ELASTIC_URL</span><span class=\"token punctuation\">:</span> (value)\n<span class=\"token punctuation\">...</span></code></pre></div>\n<p>api gateway에서 요청량 제한하니 진짜 제한되는것이 신기하다.</p>\n<p>이상한게, 여전히 프론트가 모바일에서 안 뜨길래 찾아봤더니, react <code class=\"language-text\">BrowserHistory</code>사용이 문제가 될 수 있으므로 <code class=\"language-text\">HashHistory</code>를 쓰라는  글이 있었지만, 분명 데스크탑에서는 잘 떴기 때문에 이게 원인이 아니라고 생각했는데 정말이었다. 알고보니 데스크탑 크롬에서도 표시가 안 되었고, 콘솔을 보니 <code class=\"language-text\">website.reduxDevTools</code>가 없다는 에러가 표시되고 있었다. deprecate된다는 걸 전부터 보긴 했는데 아예 없앤것이거나, 설치가 안됬을때에 대한 코드 커버가 안되있거나 한거같은데, 그냥 귀찮아서 <code class=\"language-text\">redux-devtools-extension</code> 패키지를 설치해서 적용하여 해결했다.</p>\n<p>이제 모바일도 잘 들어가진다. 다만 반응형이라기엔 아쉬운 상태.</p>\n<p>할 것:</p>\n<ul>\n<li>반응형</li>\n<li>영어 검색 개선 (같은 단어 합치기)</li>\n<li>영어 검색시 5개에서 여러개로 늘리기</li>\n<li>회원 및 개인 단어장 기능. 이거 하려면 또 한 세월 걸릴 듯.</li>\n</ul>\n<p>등..</p>\n<p>그런데 이대로 <code class=\"language-text\">test2-backup</code> 브랜치를 진행해버리면 히스토리가 너무 더러워져버린다.</p>\n<h2 id=\"tags\"><a href=\"#tags\" aria-hidden class=\"anchor\"><svg aria-hidden=\"true\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>tags</h2>\n<p>  #vocabo, #cloud</p>","id":"0e8f8095-17a2-5ffd-b037-c31a822f4006","fields":{"slug":"vocabo-배포-마무리"},"frontmatter":{"date":"2019-05-30","title":"vocabo 배포 마무리","category":"vocabo","tags":["project/vocabo"],"banner":"/assets/bg/1.jpg"},"timeToRead":1}}}